{"timestamp": 1698746170.024141, "stored_source_code": "# add default values for parameters here\n# Efficient estimation techniques for Variational Quantum Simulation\n## Introduction\n$\\newcommand{\\ket}[1]{\\left|#1\\right>} \\newcommand{\\bra}[1]{\\left<#1\\right|}$\nThis notebook's purpose is to introduce the concept of classical shadow estimation, as well as its use in **VQS** (**V**ariational **Q**uantum **S**imulation). This technique, introduced in [this article by Huang, Kueng and Preskill](https://arxiv.org/abs/2002.08953), is used for efficiently estimating multiple observables, and is extremely powerful in that regard, asymptotically reaching theoretical lower bounds of quantum information theory regarding the number of required samples of a given state for estimation ([see here for details](https://arxiv.org/abs/2101.02464)). \n\nThe primary goal of this notebook is to estimate the groundstate energy of the $H_2$ molecule, using a VQS. We will first implement the method of random classical shadows in Python. Then, we'll introduce its derandomized counterpart, which is particularly useful in our setting. We'll finally describe the VQS, and benchmark the estimation methods we introduced for computing the molecule's energy. This notebook draws some inspiration from [this PennyLane Jupyter notebook](https://pennylane.ai/qml/demos/tutorial_classical_shadows.html) on quantum machine learning and classical shadows.\n## Random classical shadows\n### Main ideas and implementation\nClassical shadow estimation relies on the fact that for a particular\nchoice of measurement, we can efficiently store snapshots of the state\nthat contain enough information to accurately predict linear functions\nof observables.\n\nLet us consider an $n$-qubit quantum state $\\rho$ (prepared by a\npulse sequence) and apply a random unitary $U$ to the state:\n\n$$\\rho \\to U \\rho U^\\dagger.$$\n\nNext, we measure in the computational basis and obtain a bit string of\noutcomes $|b\\rangle = |0011\\ldots10\\rangle$. If the unitaries $U$ are\nchosen at random from a particular ensemble, then we can store the\nreverse operation $U^\\dagger |b\\rangle\\langle b| U$ efficiently in\nclassical memory. We call this a *snapshot* of the state. Moreover, we\ncan view the average over these snapshots as a measurement channel:\n\n$$\\mathbb{E}\\left[U^\\dagger |b\\rangle\\langle b| U\\right] = \\mathcal{M}(\\rho).$$\n\nWe restrict ourselves to unitary ensembles that define a tomographically complete set of\nmeasurements (i.e $\\mathcal{M}$ is invertible), therefore :\n\n$$\\rho = \\mathbb{E}\\left[\\mathcal{M}^{-1}\\left(U^\\dagger |b\\rangle\\langle b| U \\right)\\right].$$\n\nIf we apply the procedure outlined above $N$ times, then the collection\nof inverted snapshots is what we call the *classical shadow*\n\n$$S(\\rho,N) = \\left\\{\\hat{\\rho}_1= \\mathcal{M}^{-1}\\left(U_1^\\dagger |b_1\\rangle\\langle b_1| U_1 \\right)\n,\\ldots, \\hat{\\rho}_N= \\mathcal{M}^{-1}\\left(U_N^\\dagger |b_N\\rangle\\langle b_N| U_N \\right)\n\\right\\}.$$\n\nSince the shadow approximates $\\rho$, we can now estimate **any**\nobservable with the empirical mean:\n\n$$\\langle O \\rangle = \\frac{1}{N}\\sum_i \\text{Tr}{\\hat{\\rho}_i O}.$$\n\nWe will be using a median-of-means procedure in practice.\nWe start by defining several useful quantities, such as the unitary matrices associated with Pauli measurements : the Hadamard matrix, change of basis from $\\{\\ket{0}, \\ket{1}\\}$ to the eigenbasis of $\\sigma_X$, $\\{\\ket{+}, \\ket{-}\\}$, and its $\\sigma_Y, \\sigma_Z$ counterparts. We will then draw randomly from this tomographically complete set of $3$ unitaries.\n\nNote that we will need $4$ qubits for our VQS problem : we will explain the mapping from the molecule to qubits later.\nimport numpy as np\nimport qutip\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\nfrom pulser import Register, Sequence, Pulse\nfrom pulser.devices import Chadoq2\nfrom pulser_simulation import QutipEmulator\nnum_qubits = 4\nzero_state = qutip.basis(2, 0).proj()\none_state = qutip.basis(2, 1).proj()\nhadamard = 1 / np.sqrt(2) * qutip.Qobj([[1.0, 1.0], [1.0, -1.0]])\nh_mul_phase = qutip.Qobj(np.array([[1.0, 1], [1.0j, -1.0j]])) / np.sqrt(2)\nunitary_ensemble = [hadamard, h_mul_phase, qutip.qeye(2)]\n\ng = qutip.basis(2, 1)\nr = qutip.basis(2, 0)\nn = r * r.dag()\n\nsx = qutip.sigmax()\nsy = qutip.sigmay()\nsz = qutip.sigmaz()\n\ngggg = qutip.tensor([g, g, g, g])\nggrr = qutip.tensor([g, g, r, r])\nWe first define a function that spits out a random bitstring sampled from a given density matrix.\ndef measure_bitstring(state):\n    \"\"\"Auxiliary function that returns a bitstring according to the measure of a quantum state.\"\"\"\n    probs = np.real(state.diag())\n    probs /= np.sum(probs)\n    x = np.nonzero(np.random.multinomial(1, probs))[0][0]\n    bitstring = np.binary_repr(x, num_qubits)\n    return bitstring\nWe will need to compute the number of shadows needed given :\n\n* A list of observables $o_i$\n* Desired precision on expectation values $\\epsilon$ : if $\\tilde{o}_i$ is the estimated expectation value for observable $o_i$, we wish for $|Tr(o_i \\rho) - \\tilde{o}_i| \\leq \\epsilon$\n* Failure probability $\\delta$ : we wish for the above equation to be satisfied with probability $1-\\delta$\n\nPrecise formulae are given in [Huang et al.](https://arxiv.org/abs/2002.08953)\nThe integer $K$ returned by the function will serve as the number of blocks in our median of means procedure afterwards.\ndef compute_shadow_size(delta, epsilon, observables):\n    \"\"\"Helper function.\n\n    Computes both the number of shadows needed as well as the size of blocks needed\n    for the median_of_means method in order to approximate the expectation value of M\n    (linear) observables with additive error epsilon and fail probability delta.\n\n    Args:\n        delta (float): Failure probability.\n        epsilon (float): Additive error on expectation values.\n        observables (list[qutip.Qobj]): Observables the expectation value of which is to be computed.\n    \"\"\"\n    M = len(observables)\n    K = 2 * np.log(2 * M / delta)\n    shadow_norm = (\n        lambda op: np.linalg.norm(\n            op - np.trace(op) / 2 ** int(np.log2(op.shape[0])), ord=np.inf\n        )\n        ** 2\n    )\n    # Theoretical number of shadows per cluster in the median of means procedure :\n    # N = 34 * max(shadow_norm(o) for o in observables) / epsilon ** 2\n    # We use N = 20 here to allow for quick simulation\n    N = 20\n    return int(np.ceil(N * K)), int(K)\nNext, we design a function that returns snapshots (bitstrings) of the rotated state as well as the sampled unitaries used to rotate the state $\\rho$.\ndef calculate_classical_shadow(rho, shadow_size):\n    \"\"\"\n    Given a state rho, creates a collection of snapshots consisting of a bit string\n    and the index of a unitary operation.\n\n    Returns:\n        Tuple of two numpy arrays. The first array contains measurement outcomes as bitstrings\n        while the second array contains the index for the sampled Pauli's (0,1,2=X,Y,Z).\n    \"\"\"\n    # sample random Pauli measurements uniformly\n    unitary_ids = np.random.randint(0, 3, size=(shadow_size, num_qubits))\n    outcomes = []\n    for ns in range(shadow_size):\n        unitmat = qutip.tensor(\n            [unitary_ensemble[unitary_ids[ns, i]] for i in range(num_qubits)]\n        )\n        outcomes.append(measure_bitstring(unitmat.dag() * rho * unitmat))\n\n    # combine the computational basis outcomes and the sampled unitaries\n    return (outcomes, unitary_ids)\nWe then reconstruct an estimate of the quantum state from the sampled bitstrings, using the inverse quantum channel $\\mathcal{M}^{-1}$ defined above. In the particular case of Pauli measurements, we can actually compute the inverse channel : \n\n$$\\mathcal{M}^{-1} = \\otimes_{i=1}^n (3 U_i \\ket{b_i}\\bra{b_i} U^\\dagger_i - \\mathbb{1}_2)$$\n\nwhere $i$ runs over all qubits : $\\ket{b_i}$, $b_i \\in \\{0,1\\}$, is the single-bit snapshot of qubit $i$ and $U_i$ is the sampled unitary corresponding to the snapshot, acting on qubit $i$.\ndef snapshot_state(outcome_ns, unitary_ids_ns):\n    \"\"\"\n    Reconstructs an estimate of a state from a single snapshot in a shadow.\n\n    Implements Eq. (S44) from https://arxiv.org/pdf/2002.08953.pdf\n\n    Args:\n        outcome_ns: Bitstring at ns\n        unitary_ids_ns: Rotation applied at ns.\n\n    Returns:\n        Reconstructed snapshot.\n    \"\"\"\n    state_list = []\n\n    for k in range(num_qubits):\n        op = unitary_ensemble[unitary_ids_ns[k]]\n        b = zero_state if outcome_ns[k] == \"0\" else one_state\n        state_list.append(3 * op * b * op.dag() - qutip.qeye(2))\n\n    return qutip.tensor(state_list)\nWe finally write a median of means procedure. We feed it an observable, the list of snapshots computed above and the number of blocks needed. It returns the median of the means of the observable acting on the snapshots in each block.\ndef _median_of_means(obs, snap_list, K):\n    if K > len(snap_list):  # preventing the n_blocks > n_observations\n        K = int(np.ceil(len(snap_list) / 2))\n    # dividing seq in K random blocks\n    indic = np.array((list(range(K)) * int(len(snap_list) / K)))\n    np.random.shuffle(indic)\n    # computing and saving mean per block\n    means = []\n    for block in range(K):\n        states = [snap_list[i] for i in np.where(indic == block)[0]]\n        exp = qutip.expect(obs, states)\n        means.append(np.mean(exp))\n    return np.median(means)\n### Reconstructing a given quantum state\nLet us try out the efficiency of this method. We will reconstruct a given density matrix from classical shadows estimation, and observe the evolution of the trace distance between the original state and its reconstruction according to the number of shadows used.\ndef state_reconstruction(snaps):\n    return sum(snaps) / len(snaps)\nnum_qubits = 2\nshadow_size = 10000\nrho_1 = (\n    (\n        qutip.tensor([qutip.basis(2, 0), qutip.basis(2, 0)])\n        + qutip.tensor([qutip.basis(2, 0), qutip.basis(2, 1)])\n    )\n    .proj()\n    .unit()\n)\nprint(\"Original density matrix :\")\nprint(rho_1.full())\noutcomes, unitary_ids = calculate_classical_shadow(rho_1, shadow_size)\nsnapshots = [\n    snapshot_state(outcomes[ns], unitary_ids[ns]) for ns in range(shadow_size)\n]\nprint(\"Shadow reconstruction :\")\nprint(np.around(state_reconstruction(snapshots).full(), 2))\n\ndist = np.zeros(5)\nshadow_sizes = [100, 1000, 2000, 5000, 10000]\nfor i, shadow_size in enumerate(shadow_sizes):\n    outcomes, unitary_ids = calculate_classical_shadow(rho_1, shadow_size)\n    snapshots = [\n        snapshot_state(outcomes[ns], unitary_ids[ns])\n        for ns in range(shadow_size)\n    ]\n    dist[i] = qutip.tracedist(state_reconstruction(snapshots), rho_1)\nnum_qubits = 4\nplt.plot(shadow_sizes, dist)\nplt.xlabel(\"Shadow size\")\nplt.ylabel(r\"$||\\rho - \\hat{\\rho}||_1$\")\nplt.show()\nAs we can expect, the estimation gets better and better as shadow size gets larger, with about $2$% accuracy at $10000$ shadows. This mostly serves as a reality check, as we will be using classical shadows to estimate observables acting on quantum states, not to reconstruct those states.\n## Derandomized Paulis\n### Derandomization Algorithm\nRandomized classical shadows are useful when dealing with low-weight, general observables. However, suppose, as is the case when estimating the Hamiltonian of the $H_2$ molecule written as a sum of Pauli strings, that we're dealing with Pauli observables of varying weights. In this setting, choosing wisely each Pauli measurement instead of randomly drawing a basis is particularly useful : indeed, say one wants to measure observable $\\sigma_x^1 \\otimes \\sigma_x^2 \\otimes \\dots \\otimes \\sigma_x^n$. Using random rotations in each Pauli $X,Y$ or $Z$ basis and projection in the $Z$ (computational) basis, there is a probability $\\frac{1}{3^n}$ to get each measurement basis right (i.e. rotate the system using the Hadamard matrix). This is extremely unlikely and unefficient as the number of qubits goes up. [Huang et al](https://arxiv.org/abs/2103.07510) outline an interesting greedy algorithm used for choosing suitable measurement bases for the efficient estimation of $L$ $n-$qubit Pauli strings, $\\{O_i\\}$. \n\nFeeding these observables and chosen Pauli measurements {P_i} as input, the algorithm aims at optimizing a certain cost function. This function, labeled $Conf_\\epsilon(O_i, P_j)$ is such that, if $Conf_\\epsilon(O_i, P_j) \\leq \\frac{\\delta}{2}$, then the empirical averages $\\tilde{\\omega_l}$ of each Pauli observable $O_l$ will be $\\epsilon$-close to its true average $Tr(\\rho O_l)$ with probability $1-\\delta$.\nIn order to implement this cost function, we first need to design two auxiliary functions. The first one decides if a given Pauli measurement $p$ is compatible with (\"hits\") a Pauli observable $o$. This means that each time $o$ acts non-trivially on a qubit $q_i$ with Pauli matrix $\\sigma \\in \\{\\sigma_X, \\sigma_Y, \\sigma_Z\\}, \\sigma \\neq \\mathbb{1}$, $p$ acts on $q_i$ with $\\sigma$. We denote it by $o \\triangleright p$.\ndef hits(p, o, end=-1):\n    \"\"\"Determines if measurement p hits observable o\n\n    Args:\n        p (str): Pauli string in str format (ex \"XYZ\"), measurement\n        o (str): same as above, observable (ex \"11ZY\")\n        end (int): index before which to check if p hits o\n    \"\"\"\n    if end != -1:\n        o = o[:end]\n    for i, x in enumerate(o):\n        if not (x == p[i] or x == \"1\"):\n            return False\n    return True\nThe second function simply computes the number of qubits observable $o$ acts non-trivially upon.\ndef weight(o, start=0):\n    o_k = o[start:]\n    return len(o_k) - o_k.count(\"1\")\nWe now implement the conditioned cost function using these auxiliary functions. We call it \"conditioned\", since we feed it only the first $m \\times n + k$ single-qubit Pauli measurements, and average over the others, not yet determined ones.\ndef cond_conf(o, P_sharp):\n    \"\"\"Returns the (modified) conditionned expectation value of the cost function depending\n    on already chosen Paulis in P_sharp.\n\n    Args:\n        o (list[str]): list of Pauli strings to be measured\n        P_sharp (list[str]): list of already chosen Paulis\n    \"\"\"\n    # Hyperparameters : see Huang et al. for more details\n    eta = 0.9\n    nu = 1 - np.exp(-eta / 2)\n    L = len(o)\n    m = len(P_sharp) - 1  # index of last chosen Pauli string\n    k = (\n        len(P_sharp[-1]) - 1\n    )  # index of last chosen Pauli matrix in mth Pauli string\n    result = 0\n    for l in range(0, L):\n        v = 0\n        for m_prime in range(0, m):\n            v += (eta / 2) * int(hits(P_sharp[m_prime], o[l]))\n        v -= np.log(\n            1\n            - (nu / 3 ** (weight(o[l], start=k + 1)))\n            * hits(P_sharp[m], o[l], end=k + 1)\n        )\n        result += np.exp(-v)\n    return result\nFinally, we design a simple greedy algorithm which purpose is to minimize this conditioned cost function, choosing one single-qubit Pauli at a time.\ndef derandomization(M, o):\n    \"\"\"Derandomization algorithm returning best Pauli indices according to a greedy algorithm\n    that aims at minimizing the cost function above.\n\n    Args:\n        M (int): number of measurements\n        n (int): number of qubits (size of Pauli strings)\n        epsilon (float): desired accuracy on observable expectation values\n        o (list[str]): list of Pauli strings to be measured\n    \"\"\"\n    n = len(o[0])\n    P_sharp = []\n    for m in range(M):\n        P_sharp.append(\"\")\n        for k in range(n):\n            P_sharp_m = P_sharp[m]\n            P_sharp[m] += \"X\"\n            valmin = cond_conf(o, P_sharp)\n            argmin = \"X\"\n            for W in [\"Y\", \"Z\"]:\n                P_sharp[m] = P_sharp_m + W\n                val_W = cond_conf(o, P_sharp)\n                if val_W < valmin:\n                    valmin = val_W\n                    argmin = W\n            P_sharp[m] = P_sharp_m + argmin\n    return P_sharp\n### Estimating expectation values from Pauli measurements\nNow that we have our Pauli measurements, we proceed differently from randomized classical shadows, where we gave an estimate of the actual quantum channels. Here, we're only interested in the Pauli averages $\\tilde{\\omega}_l$, that we can infer from Pauli measurements $p$ that **hit** observable $o_l$. Indeed, we have the following formula :\n\n$$\\tilde{\\omega}_{l}=\\frac{1}{h\\left(\\mathbf{o}_{l} ;\\left[\\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{M}\\right]\\right)} \\sum_{m: \\mathbf{o}_{l} \\triangleright \\mathbf{p}_{m}} \\prod_{j: \\mathbf{o}_{l}[j] \\neq I} \\mathbf{q}_{m}[j]$$\n\nwhere $h\\left(\\mathbf{o}_{l} ;\\left[\\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{M}\\right]\\right)$ is the number of times a Pauli measurement $p_i$ is such that $o \\triangleright p_i$, and $\\mathbf{q}_m$ is the output of the measurement of Pauli string $p_m$ ($\\mathbf{q}_m \\in \\{\\pm 1\\}^n$).\ndef _pauli_index(letter):\n    if letter == \"X\":\n        return 0\n    elif letter == \"Y\":\n        return 1\n    else:\n        return 2\ndef pauli_string_value(x, sigma):\n    \"\"\"Returns the evaluation of a Pauli string sigma in a bitstring state $|x>$,\n    assuming the state is already rotated in the needed eigenbases of all single-qubit Paulis.\n\n    NB : Faster than using qutip.measure due to not returning the eigenstates...\n\n    Args:\n        x (str): input bitstring\n        sigma (str): input Pauli string to be measured on |x>\n    \"\"\"\n    outcomes = []\n    for i, q in enumerate(x):\n        if q == \"0\":\n            outcomes.append((sigma[i], 1))\n        else:\n            outcomes.append((sigma[i], -1))\n    return outcomes\ndef classical_shadow_derand(rho, measurements):\n    \"\"\"Returns the n-strings of \u00b11 corresponding to measurements in the input list on state rho.\n\n    Args:\n        rho (qutip.Qobj): input state as a density matrix\n        measurements (list[str]): derandomized measurement bases in which to measure state rho\n\n    Returns:\n        Tuple of two numpy arrays. The first array contains measurement outcomes as bitstrings\n        while the second array contains the index for the derandomized Pauli's (0,1,2=X,Y,Z).\n    \"\"\"\n    # Fill the unitary ids with derandomized measurements ids\n    shadow_size = len(measurements)\n    outcomes = []\n    for ns in range(shadow_size):\n        # multi-qubit change of basis\n        unitmat = qutip.tensor(\n            [\n                unitary_ensemble[_pauli_index(measurements[ns][i])]\n                for i in range(num_qubits)\n            ]\n        )\n        x = measure_bitstring(unitmat.dag() * rho * unitmat)\n        outcomes.append(pauli_string_value(x, measurements[ns]))\n    # \u00b11 strings\n    return outcomes\ndef exp_value(input_pauli, pm_strings):\n    \"\"\"Computes an estimation of the expectation value of a given Pauli string given multiple \u00b11 bitstring\n    outcomes.\n    \"\"\"\n    sum_product, cnt_match = 0, 0\n\n    for single_measurement in pm_strings:\n        not_match = False\n        product = 1\n\n        for i, pauli in enumerate(input_pauli):\n            if pauli != single_measurement[i][0] and pauli != \"1\":\n                not_match = True\n                break\n            if pauli != \"1\":\n                product *= single_measurement[i][1]\n        if not_match:\n            continue\n\n        sum_product += product\n        cnt_match += 1\n    if cnt_match == 0:\n        return f\"No measurement given for {input_pauli}\"\n    return sum_product / cnt_match\n## Variational Quantum Simulation for the $H_2$ molecule\nThe main problem with usual variational classical algorithms, the classical counterparts of VQS, is computing the value of the $2^n \\times 2^n$ matrix on the output state vector $\\bra{\\psi}H\\ket{\\psi}$ after each loop of the algorithm, which grows exponentially in the size of the system. The purpose of VQS algorithms is to offer a solution which time complexity only grows polynomially, thanks to reading all the important properties on the quantum state. Therefore, we need accurate and efficient methods to estimate these properties, which we'll present afterwards.\n\nFor now, let's focus on what makes a VQS algorithm, specifically for computing the groundstate energy of the $H_2$ molecule.\n### Jordan-Wigner Hamiltonian (cost function)\nWe need to write the Hamiltonian in a way that's compatible with the formalism of quantum computing. We first second-quantize the Hamiltonian, obtaining an expression in terms of fermionic operators $a, a^\\dagger$. Then, we use the Jordan-Wigner transformation, which maps the fermionic operators to Pauli matrices. We obtain the Hamiltonian below, acting on $4$ qubits, decomposed in terms of the coefficients in front of the Pauli matrices.\n\n[This article by Seeley et al.](https://math.berkeley.edu/~linlin/2018Spring_290/SRL12.pdf) gives us the value of \n$H_{JW}$.\n$$H_{J W}=-0.81261 \\mathbb{1}+0.171201 \\sigma_{0}^{z}+0.171201 \\sigma_{1}^{z}-0.2227965 \\sigma_{2}^{z} \\\\\n-0.2227965 \\sigma_{3}^{z} +0.16862325 \\sigma_{1}^{z} \\sigma_{0}^{z}+0.12054625 \\sigma_{2}^{z} \\sigma_{0}^{z} \\\\\n+0.165868 \\sigma_{2}^{z} \\sigma_{1}^{z}+0.165868 \\sigma_{3}^{z} \\sigma_{0}^{z} +0.12054625 \\sigma_{3}^{z}\\sigma_{1}^{z} \\\\\n+0.17434925 \\sigma_{3}^{z} \\sigma_{2}^{z}-0.04532175 \\sigma_{3}^{x} \\sigma_{2}^{x} \\sigma_{1}^{y} \\sigma_{0}^{y}\\\\\n+0.04532175 \\sigma_{3}^{x} \\sigma_{2}^{y} \\sigma_{1}^{y} \\sigma_{0}^{x}+0.04532175 \\sigma_{3}^{y} \\sigma_{2}^{x}\n\\sigma_{1}^{x} \\sigma_{0}^{y} -0.04532175 \\sigma_{3}^{y} \\sigma_{2}^{y} \\sigma_{1}^{x} \\sigma_{0}^{x}$$\ndef pauli(positions=[], operators=[]):\n    op_list = [\n        operators[positions.index(j)] if j in positions else qutip.qeye(2)\n        for j in range(num_qubits)\n    ]\n    return qutip.tensor(op_list)\ncoeff_fact = [\n    0.81261,\n    0.171201,\n    0.2227965,\n    0.16862325,\n    0.174349,\n    0.12054625,\n    0.165868,\n    0.04532175,\n]\n\npaulis = [\n    pauli(),\n    pauli([0], [sz]) + pauli([1], [sz]),\n    pauli([2], [sz]) + pauli([3], [sz]),\n    pauli([1, 0], [sz, sz]),\n    pauli([3, 2], [sz, sz]),\n    pauli([2, 0], [sz, sz]) + pauli([3, 1], [sz, sz]),\n    pauli([2, 1], [sz, sz]) + pauli([3, 0], [sz, sz]),\n    pauli([3, 2, 1, 0], [sx, sx, sy, sy])\n    + pauli([3, 2, 1, 0], [sy, sy, sx, sx]),\n    pauli([3, 2, 1, 0], [sx, sy, sy, sx])\n    + pauli([3, 2, 1, 0], [sy, sx, sx, sy]),\n]\n# H2 Molecule : 4 qubits in Jordan-Wigner mapping of the Hamiltonian\na = 10\nreg = Register.from_coordinates(\n    [\n        [0, 0],\n        [a, 0],\n        [0.5 * a, a * np.sqrt(3) / 2],\n        [0.5 * a, -a * np.sqrt(3) / 2],\n    ]\n)\nreg.draw()\nLet us keep the exact ground-state energy of the molecule for future reference, by diagonalizing it exactly - this is possible for such a small system, however, this quickly becomes an intractable problem for large molecules.\ndef cost_hamiltonian_JW():\n    H = (\n        -coeff_fact[0] * paulis[0]\n        + coeff_fact[1] * paulis[1]\n        - coeff_fact[2] * paulis[2]\n        + coeff_fact[3] * paulis[3]\n        + coeff_fact[4] * paulis[4]\n        + coeff_fact[5] * paulis[5]\n        + coeff_fact[6] * paulis[6]\n        - coeff_fact[7] * paulis[7]\n        + coeff_fact[7] * paulis[8]\n    )\n    return H\n\n\nglobal H\nH = cost_hamiltonian_JW()\nexact_energy, ground_state = cost_hamiltonian_JW().groundstate()\nprint(exact_energy)\n### Quantum Loop (VQS)\nMuch like in the *Using QAOA to solve a QUBO problem* notebook, we will use a mixed classical-quantum approach for minimizing the energy. The quantum part will do the exploration in Hilbert space, according to a certain set of parameters $\\theta_i, \\tau_j$, and the classical part will find the optimal parameters given the value of the energy after each loop. For now, we will ignore sampling problems and simply compute the exact expectation value of $H_{JW}$. See [this article by Xiao Yuan et al.](https://arxiv.org/abs/1812.08767) for details about VQS algorithms.\nTwo mixing Hamiltonians are used for the exploration of the solution space :\n$H_1 = \\hbar / 2 \\sum_i \\sigma_i^x + \\sum_{j<i}\\frac{C_6}{|\\textbf{r}_i-\\textbf{r}_j|^{6}} \\hat n_i \\hat n_j$ and $H_2 = H_1 + \\hbar / 2 \\sum_i \\sigma_i^z$.\nWe apply them repeatedly one after the other in $p$ layers. In total, $2p$ unitaries $U(\\theta_i, H_1) = \\exp(-i \\theta_i H_1)$ and $U(\\tau_i, H_2) = \\exp(-i \\tau_i H_2)$ act on the initial state to produce state $|\\Psi(\\theta, \\tau)\\rangle$ and measure $H_{JW}$.\ndef quantum_loop(param, in_state, r=reg):\n    \"\"\"\n    Args:\n        param (np.array): time parameters for each mixing Hamiltonian. There are 2p time parameters in param.\n        in_state (qubit.Qobj): initial state.\n    \"\"\"\n    seq = Sequence(r, Chadoq2)\n    seq.declare_channel(\"ch0\", \"rydberg_global\")\n    middle = len(param) // 2\n\n    for tau, t in zip(param[middle:], param[:middle]):\n        pulse_1 = Pulse.ConstantPulse(tau, 1.0, 0, 0)\n        pulse_2 = Pulse.ConstantPulse(t, 1.0, 1.0, 0)\n        seq.add(pulse_1, \"ch0\")\n        seq.add(pulse_2, \"ch0\")\n\n    seq.measure(\"ground-rydberg\")\n    simul = QutipEmulator.from_sequence(seq, sampling_rate=0.05)\n    simul.set_initial_state(in_state)\n    results = simul.run()\n    return results.expect([H])[-1][-1]\n\n\ndef loop_JW(param, in_state):\n    res = minimize(\n        quantum_loop,\n        param,\n        method=\"Nelder-Mead\",\n        args=in_state,\n        options={\"return_all\": True, \"maxiter\": 200, \"adaptive\": True},\n    )\n    return res\nWe choose to act on the quantum states with $5$ layers of noncommuting mixing Hamiltonians, and an initial set of parameters such that pulses with Hamiltonian $H_1$ last $2\\mu s$, and those with $H_2$ last $4\\mu s$.\n# Setup for VQS\nlayers = 5\nparam = [2000] * layers + [4000] * layers\nWe now obtain the ground-state energy :\nimport warnings\n\n# Ignore the warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nloop_ising_results = loop_JW(param, gggg)\nprint(loop_ising_results.fun, exact_energy)\nAs we can see, it's not so far off, since we're about $2$% off from the exact value. Adding more layers, tweaking the mixing Hamiltonians or the initial parameters can help with the accuracy. \n\nLet's see how well the optimizer did after each loop.\nplt.plot(\n    [quantum_loop(pars, gggg) for pars in loop_ising_results.allvecs], \"k\"\n)\nplt.axhline(exact_energy, color=\"red\")\nSeems like we can cut on calculation time by only allowing $100$ iterations, since we don't get much more accurate afterwards.\n## Estimating Jordan-Wigner $H_2$ Hamiltonian with classical shadows\n### Randomized measurements\nWe now consider the real-life problem where we don't have access to the exact value $\\bra{\\Psi(\\theta_i, \\tau_j)} H_{JW} \\ket{\\Psi(\\theta_i, \\tau_j)}$. It can be estimated with classical shadows.\nWe modify the quantum loop to add classical shadow estimation of the several Pauli strings making up the $H_{JW}$ Hamiltonian : this is the perfect setting to do so, because we have multiple Pauli strings and most of them have low weight.\ndef exp_value_JW(exp_values):\n    return (\n        -coeff_fact[0] * exp_values[0]\n        + coeff_fact[1] * exp_values[1]\n        - coeff_fact[2] * exp_values[2]\n        + coeff_fact[3] * exp_values[3]\n        + coeff_fact[4] * exp_values[4]\n        + coeff_fact[5] * exp_values[5]\n        + coeff_fact[6] * exp_values[6]\n        - coeff_fact[7] * exp_values[7]\n        + coeff_fact[7] * exp_values[8]\n    )\ndef quantum_loop_shadows(param, in_state, shadow_size=20, r=reg):\n    \"\"\"\n    Args:\n        param (np.array): time parameters for each mixing Hamiltonian. There are 2p time parameters in param.\n        in_state (qubit.Qobj): initial state.\n    \"\"\"\n    seq = Sequence(r, Chadoq2)\n    seq.declare_channel(\"ch0\", \"rydberg_global\")\n    middle = len(param) // 2\n\n    for tau, t in zip(param[middle:], param[:middle]):\n        pulse_1 = Pulse.ConstantPulse(tau, 1.0, 0, 0)\n        pulse_2 = Pulse.ConstantPulse(t, 1.0, 1.0, 0)\n        seq.add(pulse_1, \"ch0\")\n        seq.add(pulse_2, \"ch0\")\n\n    seq.measure(\"ground-rydberg\")\n    simul = QutipEmulator.from_sequence(seq, sampling_rate=0.01)\n    simul.set_initial_state(in_state)\n\n    # Classical shadow estimation\n    # Theoretical shadow size and number of clusters :\n    # shadow_size, K = compute_shadow_size(0.1, 0.5, paulis)\n    # We use K=4 to allow for quick simulation\n    K = 4\n    rho = simul.run().get_final_state().proj()\n    outcomes, unitary_ids = calculate_classical_shadow(rho, shadow_size)\n    snapshots = [\n        snapshot_state(outcomes[ns], unitary_ids[ns])\n        for ns in range(shadow_size)\n    ]\n    meds = [_median_of_means(obs, snapshots, K) for obs in paulis]\n    return exp_value_JW(meds)\n\n\ndef loop_JW_shadows(param, in_state, shadow_size=20):\n    res = minimize(\n        quantum_loop_shadows,\n        param,\n        method=\"Nelder-Mead\",\n        args=(in_state, shadow_size),\n        options={\"return_all\": True, \"maxiter\": 100, \"adaptive\": True},\n    )\n    return res\nshadow_sizes = [10, 20, 40, 60, 80, 100]\nenergies = []\nfor shadow_size in shadow_sizes:\n    energies.append(\n        abs(\n            loop_JW_shadows(param, gggg, shadow_size=shadow_size).fun\n            - exact_energy\n        )\n    )\nplt.figure(figsize=(8, 5))\nplt.xlabel(\"Shadow size\", fontsize=15)\nplt.ylabel(r\"$|\\frac{E - E_{ground}}{E_{ground}}|$\", fontsize=20)\nplt.plot(shadow_sizes, [-e / exact_energy for e in energies])\nAs could be expected, the estimation can be worse than what we got before : we added both randomness and sampling issues to the problem. Raising shadow size will allow more and more precise results. However, it can also be closer to the exact value for the same reasons.\n### Derandomized measurements\nFinally, we try out the derandomized measurements method. To implement this one, we need to decompose the Hamiltonian into individual Pauli strings, rather than group them when they share the same leading coefficient as we did before, as it reduced the number of estimations.\ncoeff_non_fact = [\n    -0.81261,\n    0.171201,\n    0.171201,\n    -0.2227965,\n    -0.2227965,\n    0.16862325,\n    0.174349,\n    0.12054625,\n    0.12054625,\n    0.165868,\n    0.165868,\n    -0.04532175,\n    -0.04532175,\n    0.04532175,\n    0.04532175,\n]\n\npaulis_str = [\n    \"1111\",\n    \"Z111\",\n    \"1Z11\",\n    \"11Z1\",\n    \"111Z\",\n    \"ZZ11\",\n    \"11ZZ\",\n    \"Z1Z1\",\n    \"1Z1Z\",\n    \"1ZZ1\",\n    \"Z11Z\",\n    \"YYXX\",\n    \"XXYY\",\n    \"XYYX\",\n    \"YXXY\",\n]\ndef exp_value_JW_non_fact(outcomes):\n    return sum(\n        [\n            c * exp_value(sigma, outcomes)\n            for c, sigma in zip(coeff_non_fact, paulis_str)\n        ]\n    )\nThen, we ask the derandomization algorithm to return $60$ suitable Pauli measurements regarding our input Pauli observables. $60$ is arbitrary, but is small enough that the algorithm runs quickly and large enough that it gives good results.\nmeasurements = derandomization(60, paulis_str)\nprint(\n    f\"ZZZZ measurements : {measurements.count('ZZZZ')}, XXYY measurements : {measurements.count('XXYY')}, \"\n    + f\"YXXY measurements : {measurements.count('YXXY')}, XYYX measurements : {measurements.count('XYYX')}, \"\n    + f\"YYXX measurements : {measurements.count('YYXX')} : total = 60 measurements\"\n)\nAs we can see, since all Pauli observables appearing in the Jordan-Wigner Hamiltonian involving the $Z$-basis never involve another basis, we find that it is always worth it to measure Pauli string $ZZZZ$ rather than $ZZZX$, or $ZYZZ$, etc. This is a sign that our cost function is doing its job !\ndef quantum_loop_derand(param, in_state, r=reg):\n    \"\"\"\n    Args:\n        param (np.array): time parameters for each mixing Hamiltonian. There are 2p time parameters in param.\n        in_state (qubit.Qobj): initial state.\n    \"\"\"\n    seq = Sequence(r, Chadoq2)\n    seq.declare_channel(\"ch0\", \"rydberg_global\")\n    middle = len(param) // 2\n\n    for tau, t in zip(param[middle:], param[:middle]):\n        pulse_1 = Pulse.ConstantPulse(tau, 1.0, 0, 0)\n        pulse_2 = Pulse.ConstantPulse(t, 1.0, 1.0, 0)\n        seq.add(pulse_1, \"ch0\")\n        seq.add(pulse_2, \"ch0\")\n\n    seq.measure(\"ground-rydberg\")\n    simul = QutipEmulator.from_sequence(seq, sampling_rate=0.05)\n    simul.set_initial_state(in_state)\n\n    # Classical shadow estimation\n    rho = simul.run().get_final_state().proj()\n    outcomes = classical_shadow_derand(rho, measurements)\n    return exp_value_JW_non_fact(outcomes)\n\n\ndef loop_JW_derand(param, in_state):\n    res = minimize(\n        quantum_loop_derand,\n        param,\n        method=\"Nelder-Mead\",\n        args=in_state,\n        options={\"return_all\": True, \"maxiter\": 150, \"adaptive\": True},\n    )\n    return res\nmeasurement_sizes = [20, 30, 40, 60, 80, 100]\nenergies_derand = []\nfor meas_size in measurement_sizes:\n    measurements = derandomization(meas_size, paulis_str)\n    energies_derand.append(\n        abs(loop_JW_derand(param, gggg).fun - exact_energy) / abs(exact_energy)\n    )\nplt.figure(figsize=(8, 5))\nplt.xlabel(\"Measurement size\", fontsize=15)\nplt.ylabel(r\"$|\\frac{E - E_{ground}}{E_{ground}}|$\", fontsize=20)\nplt.plot(measurement_sizes, energies_derand)\nWe consistently obtain accurate results using this derandomized technique, and we obtain them far quicker than when dealing with randomized classical shadows. For roughly the same number of samples ($\\sim 60$ for each method, be it for shadow size or number of measurements), we experience much less computing time using the derandomized method. This was to be expected : by restricting the observables to Pauli strings, we allow for efficient estimation that can be easily computed in $O(M\\times n)$, as well as remove randomness problematic with higher-weight observables (such as $YYXX$ or $YXXY$).\n\nNote that we obtain $2\\%$ accuracy after about $50$ $Z-$ basis measurements (fluorescence) of the output state, rotated before each sampling in the bases returned by the derandomization algorithm.", "params": {}}